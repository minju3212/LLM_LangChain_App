{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161c0025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f448c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " '\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is a high-level Python library for integrating language models '\n",
      " 'into applications, enabling developers to focus on their core tasks without '\n",
      " 'being overwhelmed by the low-level details of how to interact with them. It '\n",
      " 'provides abstractions for interacting with various model types (like '\n",
      " 'prompting, data fetching, and more), handling common interfaces (APIs and '\n",
      " 'clients), and managing complexity around API key management, logging, and '\n",
      " 'logging APIs. LangChain makes it easier to create custom applications that '\n",
      " 'can use any kind of language model without needing to understand the '\n",
      " 'intricacies of how they work under the hood.\\n'\n",
      " '\\n'\n",
      " 'Key features of LangChain include:\\n'\n",
      " '\\n'\n",
      " '1. **Model Management**: A simple way to switch between different model '\n",
      " 'types (like GPT-3, BERT, etc.) and use them within your application.\\n'\n",
      " '2. **Data Handling**: Simplifies interacting with external data sources '\n",
      " 'using an API client.\\n'\n",
      " '3. **Error Management**: Provides a unified API for error logging and '\n",
      " 'handling, making it easier to diagnose issues during development.\\n'\n",
      " '4. **Documentation and Tutorials**: LangChain includes documentation and '\n",
      " 'sample code for quick start guidance.\\n'\n",
      " '\\n'\n",
      " 'By abstracting away the complexities of working with language models, '\n",
      " 'LangChain allows developers to focus on building applications that leverage '\n",
      " 'these powerful tools. It is particularly useful in industries like chatbots, '\n",
      " 'customer service, content generation, and more where custom applications may '\n",
      " 'need to interact with external data sources or multiple model types.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90636bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking \"파이썬은 무엇인가요?\" which translates to \"What is Python?\" in English. I need to provide a detailed and accurate answer.\n",
      "\n",
      "First, I should start by defining Python as a programming language. Mention its creation, creator, and the year it was released. Then, highlight its key features like simplicity, readability, and versatility. It's important to note that Python is widely used in various fields such as web development, data analysis, artificial intelligence, and more.\n",
      "\n",
      "I should also include some basic information about its syntax, like how it uses indentation instead of braces, and the significance of the \"P\" in Python (for \"Python\" as in \"Programming Language\"). Maybe mention the community and resources available, like the official website and documentation.\n",
      "\n",
      "Additionally, it's good to touch on some popular libraries and frameworks that are part of Python, such as Django for web development, NumPy and pandas for data analysis, and TensorFlow or PyTorch for machine learning. Highlighting its popularity and the fact that it's open-source can add value.\n",
      "\n",
      "I need to make sure the answer is clear and not too technical, but still informative. Avoid jargon unless necessary, and explain terms like \"interpreted language\" and \"cross-platform\" if they're relevant. Also, mention that Python is used in both small scripts and large-scale applications.\n",
      "\n",
      "Check for any recent updates or changes in Python's features, but since the user is asking a general question, sticking to the basics is better. Maybe end with a positive note about its usefulness and the community support.\n",
      "</think>\n",
      "\n",
      "파이썬은 프로그래밍 언어로, 1991년에 간단하고 읽기 쉬운 문법을 특징으로 한 Python 라이브러리가 개발되었습니다. 이 언어는 간결한 문법과 높은 생산성으로 인해 다양한 분야에서 널리 사용됩니다.  \n",
      "\n",
      "### 주요 특징  \n",
      "1. **간결한 문법**: `if` 문에 `else`를 붙이기 위한 `if` 문법과 `for` 문에 `in`을 붙이기 위한 `for` 문법 등이 있지만, 들여쓰기로 문장을 구분하는 방식으로, 코드의 읽기 쉬움을 강조합니다.  \n",
      "2. **다양한 활용 분야**:  \n",
      "   - **웹 개발**: Django, Flask 등 라이브러리 활용  \n",
      "   - **데이터 분석**: Pandas, NumPy 등 패키지  \n",
      "   - **인공지능(AI)**: TensorFlow, PyTorch 등  \n",
      "   - **게임 개발**: Pygame, Godot 등  \n",
      "   - **데이터 과학**: Scikit-learn, Matplotlib 등  \n",
      "3. **정해진 언어**: 구문에 대한 규칙이 명확하며, 오류를 쉽게 찾아볼 수 있습니다.  \n",
      "4. **개방성**: 오픈소스 프로젝트로, 사용자들이 소스 코드를 볼 수 있으며, 커뮤니티가 빠르게 기능을 추가합니다.  \n",
      "\n",
      "### 주요 이점  \n",
      "- **다중 언어 지원**: Python은 C, C++, Java, JavaScript 등 다양한 언어를 지원합니다.  \n",
      "- **크로스 플랫폼**: 운영 체제나 소프트웨어에 관계없이 사용 가능합니다.  \n",
      "- **다양한 라이브러리**: 라이브러리의 수는 수백 개로, 다양한 목적에 사용 가능합니다.  \n",
      "\n",
      "### 사용 예  \n",
      "```python\n",
      "# 단순한 계산\n",
      "print(5 + 3)  # 출력: 8\n",
      "\n",
      "# 반복\n",
      "for i in range(5):\n",
      "    print(i)  # 0, 1, 2, 3, 4\n",
      "\n",
      "# 조건문\n",
      "if i % 2 == 0:\n",
      "    print(\"짝수\")\n",
      "else:\n",
      "    print(\"홀수\")\n",
      "```\n",
      "\n",
      "### 결론  \n",
      "파이썬은 간결하고 유용한 언어로, 프로그래머들이 다양한 분야에서 쉽게 활용할 수 있습니다. 주요 라이브러리와 커뮤니티의 지원이 강점이며, 신기술과 빠르게 발전하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 qwen2.5:1.5b 모델 로드\n",
    "#llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "#qwen3:1.7b\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dca27d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I'll compare the whole number parts of both numbers. Both 9.9 and 9.11 have a whole number part of 9, so they are equal in that aspect.\n",
      "\n",
      "Next, I'll look at the decimal parts to determine which number is larger. The decimal part of 9.9 is .9, while that of 9.11 is .11.\n",
      "\n",
      "To make it easier to compare, I'll rewrite .9 as .90. Now, comparing .90 and .11, I can see that the tenths place is higher in .90 (9) than in .11 (1). Therefore, 9.90 is greater than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, follow these steps:\n",
      "\n",
      "1. **Compare the Whole Number Parts:**\n",
      "   - Both numbers have the same whole number part: **9**.\n",
      "\n",
      "2. **Compare the Decimal Parts:**\n",
      "   - For 9.9, the decimal part is **0.9**.\n",
      "   - For 9.11, the decimal part is **0.11**.\n",
      "\n",
      "3. **Convert for Comparison:**\n",
      "   - Write 0.9 as **0.90** to match the number of decimal places in 0.11.\n",
      "   - Now, compare **0.90** and **0.11**:\n",
      "     - The tenths place (first digit after the decimal) is **9** for both numbers.\n",
      "     - Since **9 > 1**, it follows that **0.90 > 0.11**.\n",
      "\n",
      "4. **Conclusion:**\n",
      "   - Since the whole number parts are equal and the decimal part of **9.9** (0.90) is greater than that of **9.11** (0.11), we conclude that:\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9 > 9.11}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e9e878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "First, I'll compare the whole number parts of both numbers. Both 9.9 and 9.11 have a whole number part of 9, so they are equal in that aspect.\n",
       "\n",
       "Next, I'll look at the decimal parts to determine which number is larger. The decimal part of 9.9 is .9, while that of 9.11 is .11.\n",
       "\n",
       "To make it easier to compare, I'll rewrite .9 as .90. Now, comparing .90 and .11, I can see that the tenths place is higher in .90 (9) than in .11 (1). Therefore, 9.90 is greater than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, follow these steps:\n",
       "\n",
       "1. **Compare the Whole Number Parts:**\n",
       "   - Both numbers have the same whole number part: **9**.\n",
       "\n",
       "2. **Compare the Decimal Parts:**\n",
       "   - For 9.9, the decimal part is **0.9**.\n",
       "   - For 9.11, the decimal part is **0.11**.\n",
       "\n",
       "3. **Convert for Comparison:**\n",
       "   - Write 0.9 as **0.90** to match the number of decimal places in 0.11.\n",
       "   - Now, compare **0.90** and **0.11**:\n",
       "     - The tenths place (first digit after the decimal) is **9** for both numbers.\n",
       "     - Since **9 > 1**, it follows that **0.90 > 0.11**.\n",
       "\n",
       "4. **Conclusion:**\n",
       "   - Since the whole number parts are equal and the decimal part of **9.9** (0.90) is greater than that of **9.11** (0.11), we conclude that:\n",
       "\n",
       "\\[\n",
       "\\boxed{9.9 > 9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e7dea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think about this step by step. \n",
      "\n",
      "First, I know that when comparing decimals, it's helpful to look at the digits from left to right. Both numbers are decimals with two digits after the decimal point. Let me write them out:\n",
      "\n",
      "9.9 is the same as 9.90, right? And 9.11 is 9.11. So, comparing them digit by digit.\n",
      "\n",
      "Starting with the whole number part: both are 9. So that's equal. Then, looking at the tenths place: the first number has 9 in the tenths place, and the second number has 1. Since 9 is greater than 1, even though the second number has more decimal places, the tenths place already shows that 9.9 is larger. \n",
      "\n",
      "Wait, but maybe I should check the decimal places more carefully. Let me break it down. \n",
      "\n",
      "9.9 is 9 + 0.9. 9.11 is 9 + 0.11. So, 0.9 is 0.90, and 0.11 is 0.11. Comparing 0.90 and 0.11, clearly 0.90 is larger. Therefore, 9.9 is larger than 9.11.\n",
      "\n",
      "But maybe I should consider if there's any trick here. For example, sometimes people might confuse the decimal places. Let me confirm. \n",
      "\n",
      "If I align them:\n",
      "\n",
      "9.90\n",
      "9.11\n",
      "\n",
      "Comparing the first decimal place: 9 vs 1. Since 9 is greater, the first number is larger. Even if the second number has more digits, the first digit in the tenths place is already larger. So, 9.9 is bigger than 9.11.\n",
      "\n",
      "Another way to think about it: 9.9 is 9.90, and 9.11 is 9.11. If I subtract 9.11 from 9.90, I get 0.79, which is positive, so 9.90 is larger. \n",
      "\n",
      "I don't think there's any other way to interpret this. The numbers are straightforward. The whole numbers are equal, but the tenths place in 9.9 is 9, which is more than 1 in the tenths place of 9.11. So, 9.9 is bigger.\n",
      "\n",
      "I guess that's it. The answer should be 9.9.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
      "- 두 수는 모두 9를 기준으로 시작합니다.  \n",
      "- **십분의 1**의 자리에서 9.9는 1보다 더 큰 값이므로, 9.9는 9.11보다 더 큰 수입니다.  \n",
      "\n",
      "**답변:** 9.9  \n",
      "**이유:** 9.9는 9.11보다 0.79 더 큰 수입니다."
     ]
    }
   ],
   "source": [
    "\n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "#model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c84aca05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think about this step by step. \n",
       "\n",
       "First, I know that when comparing decimals, it's helpful to look at the digits from left to right. Both numbers are decimals with two digits after the decimal point. Let me write them out:\n",
       "\n",
       "9.9 is the same as 9.90, right? And 9.11 is 9.11. So, comparing them digit by digit.\n",
       "\n",
       "Starting with the whole number part: both are 9. So that's equal. Then, looking at the tenths place: the first number has 9 in the tenths place, and the second number has 1. Since 9 is greater than 1, even though the second number has more decimal places, the tenths place already shows that 9.9 is larger. \n",
       "\n",
       "Wait, but maybe I should check the decimal places more carefully. Let me break it down. \n",
       "\n",
       "9.9 is 9 + 0.9. 9.11 is 9 + 0.11. So, 0.9 is 0.90, and 0.11 is 0.11. Comparing 0.90 and 0.11, clearly 0.90 is larger. Therefore, 9.9 is larger than 9.11.\n",
       "\n",
       "But maybe I should consider if there's any trick here. For example, sometimes people might confuse the decimal places. Let me confirm. \n",
       "\n",
       "If I align them:\n",
       "\n",
       "9.90\n",
       "9.11\n",
       "\n",
       "Comparing the first decimal place: 9 vs 1. Since 9 is greater, the first number is larger. Even if the second number has more digits, the first digit in the tenths place is already larger. So, 9.9 is bigger than 9.11.\n",
       "\n",
       "Another way to think about it: 9.9 is 9.90, and 9.11 is 9.11. If I subtract 9.11 from 9.90, I get 0.79, which is positive, so 9.90 is larger. \n",
       "\n",
       "I don't think there's any other way to interpret this. The numbers are straightforward. The whole numbers are equal, but the tenths place in 9.9 is 9, which is more than 1 in the tenths place of 9.11. So, 9.9 is bigger.\n",
       "\n",
       "I guess that's it. The answer should be 9.9.\n",
       "</think>\n",
       "\n",
       "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
       "- 두 수는 모두 9를 기준으로 시작합니다.  \n",
       "- **십분의 1**의 자리에서 9.9는 1보다 더 큰 값이므로, 9.9는 9.11보다 더 큰 수입니다.  \n",
       "\n",
       "**답변:** 9.9  \n",
       "**이유:** 9.9는 9.11보다 0.79 더 큰 수입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07b0d8",
   "metadata": {},
   "source": [
    "### LangGraph를 사용하여 DeepSeek 모델(추론)과 Qwen 모델(한글 응답)을 연동하기\n",
    "* poetry add langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f8eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen3:1.7b' temperature=0.7\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\\n\\n        당신의 작업은 다음과 같습니다:\\n        - 질문과 제공된 추론을 신중하게 분석하세요.\\n        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\\n        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\\n        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\\n\\n        지침:\\n        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\\n        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\\n        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\\n        - 도움이 되고 전문적인 톤을 유지하세요.\\n\\n        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        질문: {question}\\n        추론: {thinking}\\n        '), additional_kwargs={})]\n",
      "<think>\n",
      "Okay, the user is asking which is larger between 9.9 and 9.11. Let me think through this step by step.\n",
      "\n",
      "First, I notice both numbers start with 9, so the whole numbers are equal. That means I need to look at the decimal parts to determine which is bigger. \n",
      "\n",
      "The first number is 9.9, and the second is 9.11. To compare them properly, I should align the decimal places. So, 9.9 becomes 9.90 when I add a zero to the end. Now, comparing 9.90 and 9.11:\n",
      "\n",
      "Looking at the tenths place, 9.90 has a 9, while 9.11 has a 1. Since 9 is greater than 1, the tenths place tells me that 9.90 is larger than 9.11.\n",
      "\n",
      "I should make sure I didn't miss any other digits. The units place is the same for both, so that doesn't affect the comparison. The decimal parts are the key here. Adding a zero to 9.9 makes it 9.90, which is clearly larger than 9.11.\n",
      "\n",
      "So, the conclusion is that 9.9 is larger than 9.11. I need to present this in a clear, conversational way without mentioning the reasoning process. Just state the comparison and the reason based on the decimal places.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 9.9는 더 크다. 두 수는 정수 부분(9)이 같지만, 소수 부분을 비교해야 합니다. 9.9는 9.90과 같이 0을 추가해 9.90으로 바꾸면, 9.90와 9.11을 비교할 때 소수점 첫 번째 자리에서 9가 1보다 크므로 9.90이 더 크다. 따라서 9.9는 9.11보다 더 커요.\n",
      "{'question': '9.9와 9.11 중 무엇이 더 큰가요?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nBoth numbers have the same whole number part, which is 9.\\n\\nTo make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\\n\\nNow, comparing each digit from left to right:\\n\\n- The units place for both numbers is 9.\\n- In the tenths place, 9 has a 9 and 9.11 has a 1.\\n  \\nSince 9 is greater than 1 in the tenths place, 9.90 is larger than 9.11.\\n\\nTherefore, 9.9 is greater than 9.11.\\n\", 'answer': \"<think>\\nOkay, the user is asking which is larger between 9.9 and 9.11. Let me think through this step by step.\\n\\nFirst, I notice both numbers start with 9, so the whole numbers are equal. That means I need to look at the decimal parts to determine which is bigger. \\n\\nThe first number is 9.9, and the second is 9.11. To compare them properly, I should align the decimal places. So, 9.9 becomes 9.90 when I add a zero to the end. Now, comparing 9.90 and 9.11:\\n\\nLooking at the tenths place, 9.90 has a 9, while 9.11 has a 1. Since 9 is greater than 1, the tenths place tells me that 9.90 is larger than 9.11.\\n\\nI should make sure I didn't miss any other digits. The units place is the same for both, so that doesn't affect the comparison. The decimal parts are the key here. Adding a zero to 9.9 makes it 9.90, which is clearly larger than 9.11.\\n\\nSo, the conclusion is that 9.9 is larger than 9.11. I need to present this in a clear, conversational way without mentioning the reasoning process. Just state the comparison and the reason based on the decimal places.\\n</think>\\n\\n9.9와 9.11 중 9.9는 더 크다. 두 수는 정수 부분(9)이 같지만, 소수 부분을 비교해야 합니다. 9.9는 9.90과 같이 0을 추가해 9.90으로 바꾸면, 9.90와 9.11을 비교할 때 소수점 첫 번째 자리에서 9가 1보다 크므로 9.90이 더 크다. 따라서 9.9는 9.11보다 더 커요.\"}\n",
      "==> 생성된 답변: \n",
      "\n",
      "<think>\n",
      "Okay, the user is asking which is larger between 9.9 and 9.11. Let me think through this step by step.\n",
      "\n",
      "First, I notice both numbers start with 9, so the whole numbers are equal. That means I need to look at the decimal parts to determine which is bigger. \n",
      "\n",
      "The first number is 9.9, and the second is 9.11. To compare them properly, I should align the decimal places. So, 9.9 becomes 9.90 when I add a zero to the end. Now, comparing 9.90 and 9.11:\n",
      "\n",
      "Looking at the tenths place, 9.90 has a 9, while 9.11 has a 1. Since 9 is greater than 1, the tenths place tells me that 9.90 is larger than 9.11.\n",
      "\n",
      "I should make sure I didn't miss any other digits. The units place is the same for both, so that doesn't affect the comparison. The decimal parts are the key here. Adding a zero to 9.9 makes it 9.90, which is clearly larger than 9.11.\n",
      "\n",
      "So, the conclusion is that 9.9 is larger than 9.11. I need to present this in a clear, conversational way without mentioning the reasoning process. Just state the comparison and the reason based on the decimal places.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 9.9는 더 크다. 두 수는 정수 부분(9)이 같지만, 소수 부분을 비교해야 합니다. 9.9는 9.90과 같이 0을 추가해 9.90으로 바꾸면, 9.90와 9.11을 비교할 때 소수점 첫 번째 자리에서 9가 1보다 크므로 9.90이 더 크다. 따라서 9.9는 9.11보다 더 커요.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "#generation_model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.7)\n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.7)\n",
    "print(generation_model)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n",
    "\n",
    "#LangGraph에서 State 사용자정의 클래스는 노드 간의 정보를 전달하는 틀입니다. \n",
    "#노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야 할 정보를 미리 정의힙니다. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "#DeepSeek를 통해서 추론 부분까지만 생성합니다. \n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwen를 통해서 결과 출력 부분을 생성합니다.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate]) # think랑 generate는 위에 함수 이름임.\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 입력 데이터\n",
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "# invoke()를 사용하여 그래프 호출\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"==> 생성된 답변: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d80bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "add76d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "To make an accurate comparison, it's helpful to express both numbers with the same number of decimal places. This means converting 9.9 into 9.90.\n",
      "\n",
      "Now that both numbers have two decimal places, I can directly compare them digit by digit from left to right.\n",
      "\n",
      "Starting with the units place: Both numbers have a 9 in the units place, so they are equal there.\n",
      "\n",
      "Next, looking at the tenths place: The first number has a 9, and the second number has a 1. Since 9 is greater than 1, this means that 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. I need to figure out the answer. \n",
      "\n",
      "First, I remember that when comparing decimals, you start from the leftmost digit. Both numbers are straightforward. 9.9 is the same as 9.90 when you add a zero at the end, right? So, 9.90 versus 9.11. \n",
      "\n",
      "Wait, the tenths place: 9.9 has a 9 in the tenths, and 9.11 has a 1. So, 9 is bigger than 1, so even though the units are the same, the tenths place makes 9.90 larger. \n",
      "\n",
      "So, even though 9.9 is written without a decimal, it's actually 9.90. Therefore, 9.90 is bigger than 9.11. So the answer is 9.9 is larger. \n",
      "\n",
      "I need to make sure there's no confusion. Sometimes people might think that 9.9 is shorter, but in decimal terms, adding a zero doesn't change the value. So yes, 9.9 is bigger. \n",
      "\n",
      "I should explain this step by step, starting with converting both numbers to have the same decimal places, then comparing each digit. That way, the user understands the process.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 9.9는 더 큰 수입니다.  \n",
      "이 두 수는 소수점 자리 수를 맞춰서 비교하면 9.90과 9.11이 됩니다.  \n",
      "소수점 첫 번째 자리(십분의 1)에서 9가 1보다 큰값을 가질 수 있어, 9.90이 9.11보다 더 큰 수입니다.  \n",
      "따라서 9.9은 9.11보다 더 큰 수입니다.<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. I need to figure out the answer. \n",
      "\n",
      "First, I remember that when comparing decimals, you start from the leftmost digit. Both numbers are straightforward. 9.9 is the same as 9.90 when you add a zero at the end, right? So, 9.90 versus 9.11. \n",
      "\n",
      "Wait, the tenths place: 9.9 has a 9 in the tenths, and 9.11 has a 1. So, 9 is bigger than 1, so even though the units are the same, the tenths place makes 9.90 larger. \n",
      "\n",
      "So, even though 9.9 is written without a decimal, it's actually 9.90. Therefore, 9.90 is bigger than 9.11. So the answer is 9.9 is larger. \n",
      "\n",
      "I need to make sure there's no confusion. Sometimes people might think that 9.9 is shorter, but in decimal terms, adding a zero doesn't change the value. So yes, 9.9 is bigger. \n",
      "\n",
      "I should explain this step by step, starting with converting both numbers to have the same decimal places, then comparing each digit. That way, the user understands the process.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 9.9는 더 큰 수입니다.  \n",
      "이 두 수는 소수점 자리 수를 맞춰서 비교하면 9.90과 9.11이 됩니다.  \n",
      "소수점 첫 번째 자리(십분의 1)에서 9가 1보다 큰값을 가질 수 있어, 9.90이 9.11보다 더 큰 수입니다.  \n",
      "따라서 9.9은 9.11보다 더 큰 수입니다.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eda48c",
   "metadata": {},
   "source": [
    "### 2개의 모델을 연동한 코드를 Gradio를 사용하여 UI로 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de4bd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-kGdHTiMZ-py3.12\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 입력 질문: 파이썬이란?\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 9\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] generate 함수 - 질문: 파이썬이란?\n",
      "[DEBUG] generate 함수 - 추론 길이: 9\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "[DEBUG] 최종 응답 타입: <class 'str'>\n",
      "[DEBUG] 최종 응답 길이: 1088\n",
      "[DEBUG] 최종 응답 내용: <think>\n",
      "Okay, let's see. The user asked, \"파이썬은 무엇인가?\" which means \"What is Python?\" and provided the inference process. The task is to create a natural, conversational Korean answer without explicitly mentioning the inference process.\n",
      "\n",
      "First, I need to explain what Python is. It's a programming language, right? I should mention it's used for various purposes like web development, data analysis, automation, etc. Also, note that it's known for being easy to learn and read, which makes it popular among beginners. Maybe add that it's open-source and has a vast ecosystem. Keep it simple and friendly, like a chat response. Avoid technical jargon but make sure the key points are covered. Check for clarity and flow, making sure it's not too long but still informative. Alright, let's put it all together in a natural, engaging way.\n",
      "</think>\n",
      "\n",
      "파이썬은 프로그래밍 언어 중 하나로, 간결하고 읽기 쉽게 설계된 언어로, 웹 개발, 데이터 분석, 자동화, 인공지능 등 다양한 분야에서 널리 활용됩니다. 특히, 초보자도 쉽게 배워서 \"Python is easy to learn\"라는 평가를 받고 있습니다. 이 언어는 대규모 커뮤니티와 함께 다양한 라이브러리(예: NumPy, Pandas, Django 등)를 제공해 실용적인 솔루션을 만들 수 있어 인기 있는 언어로 자리 잡았습니다. 🐍\n",
      "[DEBUG] 입력 질문: 파이썬이란?\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 9\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] generate 함수 - 질문: 파이썬이란?\n",
      "[DEBUG] generate 함수 - 추론 길이: 9\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "[DEBUG] 최종 응답 타입: <class 'str'>\n",
      "[DEBUG] 최종 응답 길이: 1537\n",
      "[DEBUG] 최종 응답 내용: <think>\n",
      "Okay, let's tackle this question. The user is asking, \"What is Python?\" and the provided inference is about Python being a programming language. I need to create a natural, conversational response in Korean without explicitly mentioning the inference.\n",
      "\n",
      "First, I should recall the key points about Python. It's a high-level, general-purpose language, known for its readability. It's widely used in web development, data analysis, machine learning, and more. Also, it's interpreted, which means it's easier to debug. Features like dynamic typing and a vast standard library are important. Maybe mention the community and ease of learning as benefits.\n",
      "\n",
      "I need to structure the answer to directly address the question. Start with a simple definition, then list the main features. Use examples like web development with Django or data analysis with pandas. Highlight why it's popular—readability, ease of use, extensive libraries. Avoid technical jargon but keep it accurate. Make sure it's friendly and engaging, like a conversation. Check for clarity and flow, ensuring each point connects logically. Avoid any markdown and keep the tone professional yet approachable.\n",
      "</think>\n",
      "\n",
      "파이썬은 간결하고 읽기 쉬운 코드를 작성할 수 있는 고전적인 프로그래밍 언어입니다. 이 언어는 동적 타입 특성과 직관적인 문법으로, 데이터 분석, 웹 개발, 머신러닝 등 다양한 분야에서 널리 활용되고 있습니다. 파이썬은 간결한 코드로 복잡한 작업도 쉽게 처리할 수 있으며, 표준 라이브러리의 풍부한 기능과 커뮤니티의 빠른 발전으로 인해 개발자들에게 큰 사랑을 받고 있습니다. 예를 들어, 웹 애플리케이션을 Django로 개발하거나, 데이터 분석을 pandas로 처리할 때 파이썬의 효율성과 명확성은 매우 유리합니다. 요약하면, 파이썬은 코드를 쉽게 작성하고 유지할 수 있는 강력한 도구로, 프로그래밍 세계에서 중요한 역할을 합니다.\n",
      "[DEBUG] 입력 질문: langchain이란 무엇인가요?\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 3764\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "Okay, so I'm trying to understand what LangChain is. From the previous explanation, it seems like it's a tool that combines language models with some kind of search engine or something similar...\n",
      "[DEBUG] generate 함수 - 질문: langchain이란 무엇인가요?\n",
      "[DEBUG] generate 함수 - 추론 길이: 3764\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "Okay, so I'm trying to understand what LangChain is. From the previous explanation, it seems like it's a tool that combines language models with some kind of search engine or something similar...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "[DEBUG] 최종 응답 타입: <class 'str'>\n",
      "[DEBUG] 최종 응답 길이: 515\n",
      "[DEBUG] 최종 응답 내용: <think>\n",
      "</think>\n",
      "\n",
      "LangChain는 언어 모델과 연결된 수용기 시스템으로, 다양한 질문에 대해 정확한 답변을 제공하는 데 사용됩니다. 이는 단순한 언어 모델과 구별되며, 사용자가 입력한 질문에 대해 관련 데이터를 검색하고 처리하여 답변을 생성합니다. 예를 들어, \"아프리카의 기후 변화 상황은 어떻게 되나요?\"라고 묻면, LangChain는 모델의 지식 범위 내에서 관련 정보를 찾아내어 답변을 제공합니다. 또한, 사용자는 다양한 데이터 소스(예: 브라우저, 데이터베이스 등)와 연동하여 복잡한 문제를 해결할 수 있습니다. LangChain는 유연성과 확장성을 강조하며, 개발자들이 기존의 시스템과 쉽게 결합하여 새로운 기능을 추가할 수 있도록 설계되었습니다. 그러나 특정ly 정해진 주제나 복잡한 질문에 대한 답변이 부족할 수 있으며, 모델의 지식 범위 내에서만 처리됩니다. 이는 사용자에게 편리한 정보 접근을 제공하지만, 완전한 전문 지식을 보유하지 않는 경우 답변의 정확성에 영향을 줄 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 인코딩 강제 설정 (Jupyter 노트북 호환)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter 환경에서는 reconfigure 대신 환경변수로 처리\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter 노트북이나 다른 환경에서는 패스\n",
    "    pass\n",
    "\n",
    "# 모델 설정: 두 개의 서로 다른 모델을 사용하여 추론과 답변 생성을 수행\n",
    "# - reasoning_model: 추론을 담당하는 모델 (온도 낮음, 정확한 분석용)\n",
    "# - generation_model: 답변 생성을 담당하는 모델 (온도 높음, 창의적 응답용)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    #model=\"qwen2.5:1.5b\", \n",
    "    model=\"qwen3:1.7b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 상태(State) 정의: 그래프에서 상태를 유지하기 위한 데이터 구조\n",
    "class State(TypedDict):\n",
    "    question: str   # 사용자의 질문\n",
    "    thinking: str   # 추론 결과\n",
    "    answer: str     # 최종 답변\n",
    "\n",
    "# 개선된 프롬프트 템플릿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"당신은 한국어로 응답하는 AI 어시스턴트입니다. \n",
    "        반드시 한국어로만 답변하세요.\n",
    "        \n",
    "        당신의 작업:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 한국어 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "        \n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "        \n",
    "        중요: 반드시 한국어로만 응답하세요.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"질문: {question}\n",
    "        \n",
    "        추론 과정: {thinking}\n",
    "        \n",
    "        위 내용을 바탕으로 한국어로 답변해주세요:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"문자열이 UTF-8로 제대로 인코딩되었는지 확인하고 변환\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # 문자열이지만 인코딩 문제가 있을 수 있는 경우 처리\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # 문자열을 UTF-8로 인코딩했다가 다시 디코딩하여 정리\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeek를 통해서 추론 부분까지만 생성\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] 입력 질문: {question}\")\n",
    "    print(f\"[DEBUG] 질문 타입: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    thinking_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 추론 결과 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 길이: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 미리보기: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5를 통해서 결과 출력 부분을 생성\n",
    "def generate(state: State):\n",
    "    # question = ensure_utf8_string(state[\"question\"])\n",
    "    # thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    thinking = state[\"thinking\"]\n",
    "    \n",
    "    print(f\"[DEBUG] generate 함수 - 질문: {question}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 길이: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 미리보기: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] 프롬프트 메시지 생성 완료\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    answer_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 최종 응답 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 길이: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 내용: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# 그래프 생성 함수: 상태(State) 간의 흐름을 정의\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio 인터페이스 생성 및 실행\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "# def launch_gradio():\n",
    "#     iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "#     iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n",
    "    #launch_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
